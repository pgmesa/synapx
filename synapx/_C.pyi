"""
Synapx core C++ bindings
"""
from __future__ import annotations
import typing
__all__ = ['Tensor', 'add', 'matmul', 'mul', 'ones', 'tensor', 'zeros']
class Tensor:
    @staticmethod
    def _pybind11_conduit_v1_(*args, **kwargs):
        ...
    def __add__(self, other: Tensor) -> Tensor:
        ...
    def __matmul__(self, other: Tensor) -> Tensor:
        ...
    def __mul__(self, other: Tensor) -> Tensor:
        ...
    def add(self, other: Tensor) -> Tensor:
        ...
    def backward(self, grad: typing.Any = None) -> None:
        """
        Union[None, synapx.Tensor, torch.Tensor]: Computes the gradient of current tensor w.r.t. graph leaves.
        """
    def data(self) -> torch.Tensor:
        ...
    def dim(self) -> int:
        ...
    def is_leaf(self) -> bool:
        ...
    def matmul(self, other: Tensor) -> Tensor:
        ...
    def mul(self, other: Tensor) -> Tensor:
        ...
    def numel(self) -> int:
        ...
    def numpy(self) -> numpy.ndarray:
        ...
    def requires_grad(self) -> bool:
        ...
    def retain_grad(self) -> None:
        ...
    def retains_grad(self) -> bool:
        ...
    def torch(self) -> torch.Tensor:
        ...
    @property
    def grad(self) -> typing.Any:
        """
        Union[None, synapx.Tensor]: Gradient tensor or None
        """
    @property
    def shape(self) -> list[int]:
        ...
def add(t1: Tensor, t2: Tensor) -> Tensor:
    ...
def matmul(t1: Tensor, t2: Tensor) -> Tensor:
    ...
def mul(t1: Tensor, t2: Tensor) -> Tensor:
    ...
def ones(shape: typing.Iterable, requires_grad: bool = False, device: str = 'cpu') -> Tensor:
    ...
def tensor(data: typing.Any, requires_grad: bool = False, device: str = 'cpu') -> Tensor:
    ...
def zeros(shape: typing.Iterable, requires_grad: bool = False, device: str = 'cpu') -> Tensor:
    ...
